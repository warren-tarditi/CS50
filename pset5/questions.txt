0.  It is the longest word in the dictionaries with a 45 length.
    The word describes a lung disease caused by inhaling very fine ash and sand dust.
1. According to its man page, what does getrusage do?
        getrusage returns resource usage measures for "int who", which is always
        RUSAGE_SELF in speller.c. Specifying RUSAGE_SELF returns resource usage 
        statistics for the calling process, which is the sum of resources used 
        by all threads in the process.
2. Per that same man page, how many members are in a variable of type struct 
   rusage?
   It has 16 members
 3. Why do you think we pass before and after by reference (instead of by value) 
   to calculate, even though we’re not changing their contents?
        We pass before and after by reference (instead of by value) to the 
        function "calculate" because passing large structs by value is slow and
        takes up a lot of memory. If you're passing or returning structs by 
        value, copies of those structs will get placed on the stack, potentially
        causing stack overflow.
4. Explain as precisely as possible, in a paragraph or more, how main goes about 
   reading words from a file. In other words, convince us that you indeed 
   understand how that function’s for loop works.
   main() first ensures the arguments are valid and correct. It also defaultly use dictionaries/large if user doesn't specify which
    dictionary to use. It then calls getrusage() and calculate() to measure load times before and afterstarts reading a text
    The for loop in main() tries to read the text symbol-by-symbol, to read separate words. A word is completed
    when one or more alphabetic symbols are read, and when we encounter a non-alphanumeric character (like spacebar, or newline). If
    there is a digit, or a string tends to be too long, we traverse this word and won't check it for misspellings. If not, we will
    check our word for misspellings, reset the word index to zero, and continue the symbol reading loop. 
5. Why do you think we used fgetc to read each word’s characters one at a time 
   rather than use fscanf with a format string like "%s" to read whole words at 
   a time? Put another way, what problems might arise by relying on fscanf alone?
        fscanf with a format string like "%s" will read subsequent characters 
        until a whitespace is found (whitespace characters are considered to be 
        blank, newline and tab). Because words within the txt files sometimes 
        end with punctuation, fscanf will view them as being part of the word, 
        which complicates the reading procedure. In addition, if a longer string
        than expected was read using fscanf (e.g. a jibberish string like asdfba
        asdkdfawemflkasciaoeufalkesfasldkfjaoiwefjaslkdmcalksdfiwoefalskdfamsdcl
        asdflkasdlkmceaasdfasdf..., which could be included in a text file from
        a malicious source that is trying to break our program), we could 
        overwrite important data in memory or cause a segmentation fault.
6. Why do you think we declared the parameters for check and load as const?
    Because we don't want those parameters accidentally modified in the functions. It is sort of a safety measures
7. What data structure(s) did you use to implement your spell-checker? Be sure 
   not to leave your answer at just "hash table," "trie," or the like. Expound 
   on what’s inside each of your "nodes."
        I used a hash table. Each bucket in the has table had a linked list of
        node structs. Each node struct contains two members - 1) a char array 
        called word and 2) a pointer to a new node called next.
8.  How slow was your code the first time you got it working correctly?
At first, it was very slow because I intended to use a very simple hash function with 27 buckets which achieve O(n/27)
9. What kinds of changes, if any, did you make to your code in order to improve 
   its performance?
  I improved the performance by implementing a trie instead of hashtable so that I can get a good performance on the checking, 
    but previously I didn't convert the index of apostrophes correctly, so there's an error on the spell checking. Soon, i realized
    that I have to make a special case for apostrophes and convert it to index 26
10. I think the bottleneck is the fact that it used lot of memories, but this is the tradeoff of using a trie data structure.
    However, I still think trie data structure is the best solution for this problem